# Como Configurar e Rodar o DeepSeek-R1 Localmente: Tutorial com Docker e Docker Compose

O **DeepSeek-R1** √© um modelo avan√ßado de IA projetado para executar tarefas complexas, como racioc√≠nio l√≥gico, programa√ß√£o e matem√°tica. Rod√°-lo localmente garante maior controle sobre os dados, personaliza√ß√£o para projetos espec√≠ficos e economia com servidores externos. Neste tutorial, voc√™ aprender√° duas formas de configur√°-lo: manualmente com **Docker** e de forma automatizada com **Docker Compose**.

## Por que Rodar o DeepSeek-R1 Localmente?

- Privacidade: Seus dados permanecem no ambiente local.
- Economia: Reduz custos com servidores em nuvem.
- Personaliza√ß√£o: Facilita ajustes no ambiente e personaliza√ß√µes.

## Arquitetura e implanta√ß√£o do DeepSeek-R1

O **DeepSeek-R1** √© um modelo de IA de ponta projetado para tarefas complexas, como racioc√≠nio l√≥gico e resolu√ß√£o de problemas matem√°ticos. Ele combina **aprendizado por refor√ßo** (RL) e ajuste **supervisionado** para alcan√ßar desempenho superior em benchmarks como AIME e MATH-500. Dispon√≠vel em vers√µes completas (671B par√¢metros), vers√£o complexa e maior, e destiladas (1.5B-70B), vers√£o simplificada e reduzida de um modelo de aprendizado de m√°quina mais complexo e maior, o modelo pode ser executado localmente com ferramentas como **Ollama**, exigindo GPUs potentes ou hardware otimizado.

- **Arquitetura**: Baseado em Mixture-of-Experts (MoE), onde cada token ativa 37 bilh√µes de par√¢metros, possibilitando racioc√≠nio avan√ßado.
- **Treinamento**: Utiliza RL para melhorar capacidades de racioc√≠nio, com t√©cnicas de destila√ß√£o para criar modelos menores.
- **Implanta√ß√£o Local**: Modelos menores (1.5B-70B) podem ser executados em hardware mais modesto, enquanto vers√µes completas exigem GPUs com alto poder de processamento.

üí° **Dica**: Modelos destilados s√£o ideais para quem deseja experimentar o DeepSeek-R1 em m√°quinas locais sem depender de recursos avan√ßados.

## Configura√ß√£o Manual com Docker

Esta abordagem envolve criar e conectar os cont√™ineres manualmente.

**Considere a pasta *Ollama***

### Passo 1: Iniciar o Ollama

O **Ollama** gerencia a execu√ß√£o do modelo. Use o seguinte comando para criar e iniciar o cont√™iner::

```sh
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```
- -d: Roda o cont√™iner em segundo plano.
- -v ollama:/root/.ollama: Cria um volume chamado ollama para armazenar os dados do modelo.
- -p 11434:11434: Mapeia a porta do cont√™iner para a mesma porta no host.
- --name ollama: Nomeia o cont√™iner como ollama.

### Passo 2: Baixar o Modelo DeepSeek-R1

Depois que o cont√™iner do **Ollama** estiver em execu√ß√£o, baixe o modelo **DeepSeek-R1** com:

```sh
docker exec -it ollama ollama pull deepseek-r1:1.5b
```

üìö Explorar Outros Modelos: Para ver mais op√ß√µes, acesse a [biblioteca oficial de modelos do Ollama](https://ollama.com/library/deepseek-r1). Modelos maiores oferecem mais precis√£o, mas podem exigir mais recursos de GPU.

### Passo 3: Configurar a Interface Web

Agora, configure o **Open Web UI** para interagir com o modelo via navegador:

```console
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

- -p 3000:8080: Mapeia a porta 8080 do cont√™iner para a porta 3000 no host.
- --add-host=host.docker.internal:host-gateway: Permite que o cont√™iner acesse servi√ßos rodando no host.
- -v open-webui:/app/backend/data: Cria um volume persistente para armazenar os dados da interface web.

### Passo 4: Acessar a Interface

Abra o navegador em http://localhost:3000 para interagir com o modelo **DeepSeek-R1**.

## Automa√ß√£o com Docker Compose

Embora a configura√ß√£o manual funcione, usar **Docker Compose** automatiza a cria√ß√£o e conex√£o dos servi√ßos, tornando tudo mais simples.

### Estrutura do Projeto

Organize os arquivos do projeto com a seguinte estrutura:

```
my-deepseek/
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile             # Configura√ß√£o do Ollama
‚îú‚îÄ‚îÄ entrypoint.sh          # Script de inicializa√ß√£o
‚îú‚îÄ‚îÄ docker-compose.yml     # Configura√ß√£o do Docker Compose
‚îî‚îÄ‚îÄ README.md              # Documenta√ß√£o
```

#### Passo 1: Criar o arquivo Dockerfile

No arquivo **Dockerfile**, adicione as instru√ß√µes para criar o cont√™iner do **Ollama**:

```Dockerfile
FROM ollama/ollama:latest

# Copie o script para dentro do cont√™iner
COPY entrypoint.sh /entrypoint.sh

# Torne o script execut√°vel
RUN chmod +x /entrypoint.sh

# Configure o ENTRYPOINT para o script
ENTRYPOINT ["/bin/sh", "/entrypoint.sh"]
```

#### Passo 2: Criar o arquivo entrypoint.sh

Crie um arquivo chamado **entrypoint.sh** na raiz do projeto que ser√° o respons√°vel por carregar o modelo do **Deepseek** que iremos usar:

```sh
#!/bin/sh

# Inicie o servidor Ollama em background
ollama serve --timeout 600 &

# Aguarde o servidor iniciar
sleep 5

# Fa√ßa o pull do modelo necess√°rio (usando a vari√°vel MODEL_NAME)
ollama pull "${MODEL_NAME}"

# Mantenha o cont√™iner rodando
tail -f /dev/null
```

#### Passo 3: Criar o Arquivo docker-compose.yml

No arquivo **docker-compose.yml**, configure os servi√ßos:

```yaml
version: '3.8'

networks:
  ai_network:
    driver: bridge

services:
  ollama:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - ai_network
    environment:
      - MODEL_NAME=deepseek-r1:1.5b  # Defina o modelo aqui

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    depends_on:
      - ollama
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always
    networks:
      - ai_network

volumes:
  ollama:
  open-webui:
```

#### Passo 4: Subir os Servi√ßos

Com o Docker Compose configurado, inicie os servi√ßos com:

```bash
docker-compose up --build -d
```

#### Passo 5: Acessar a Interface

Abra http://localhost:3000 no navegador para usar o DeepSeek-R1.

#### Passo 6: Encerrar os Servi√ßos

Para desligar os cont√™ineres, use:

```bash
docker-compose down
```

## Usando a API do Ollama com o DeepSeek-R1

Voc√™ pode usar a API do **Ollama** para interagir com o modelo **DeepSeek-R1** de forma program√°tica. Aqui est√° um exemplo pr√°tico em **Python** para integrar o modelo em um aplicativo:

### Exemplo de C√≥digo em Python

#### Crie o Virtual Environment

No terminal, navegue at√© a pasta do seu projeto **src**:

```bash
python3 -m venv venv
```

#### Ative o Virtual Environment

Ative o ambiente virtual executando o comando abaixo:

```bash
source venv/bin/activate
```

#### Instale as Depend√™ncias a Partir do requirements.txt

Ap√≥s criar o ambiente virtual e ativ√°-lo, use o seguinte comando para instalar todas as depend√™ncias listadas no requirements.txt:

```bash
pip install -r requirements.txt
```
Isso instalar√° exatamente as vers√µes dos pacotes especificadas no arquivo.

#### Execute seu c√≥digo

Com o ambiente virtual ativo, rode o script:

```bash
python3 example_prompt.py # Exemplo de prompt simples
python3 example_pdf.py example.pdf # Exemplo de an√°lise de pdf passado como primeiro parametro
```

#### Desative o _Virtual Environment_

Quando terminar, voc√™ pode desativar o virtual environment com o comando:

```bash
deactivate
```

Isso retorna o terminal para o estado original.

## Conclus√£o

Com estas abordagens, voc√™ pode rodar o **DeepSeek-R1** localmente de forma eficiente e organizada. Usar vari√°veis no **Docker Compose** simplifica o carregamento de diferentes modelos no futuro. Agora √© sua vez de explorar as capacidades do **DeepSeek-R1** diretamente no seu ambiente local! üöÄ

Caso tenha d√∫vidas ou queira compartilhar sua experi√™ncia, deixe seu coment√°rio. Boa sorte com seus projetos de IA! üòä
